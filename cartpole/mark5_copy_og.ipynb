{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.activations import relu, linear\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import math\n",
    "from popgym.envs.stateless_cartpole import StatelessCartPole\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.threading.set_intra_op_parallelism_threads(2)\n",
    "tf.config.threading.set_inter_op_parallelism_threads(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(np.__config__.show())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"OMP_NUM_THREADS\"] = \"1\" # export OMP_NUM_THREADS=1\n",
    "# os.environ[\"OPENBLAS_NUM_THREADS\"] = \"1\" # export OPENBLAS_NUM_THREADS=1\n",
    "# os.environ[\"MKL_NUM_THREADS\"] = \"1\" # export MKL_NUM_THREADS=1\n",
    "# os.environ[\"VECLIB_MAXIMUM_THREADS\"] = \"1\" # export VECLIB_MAXIMUM_THREADS=1\n",
    "# os.environ[\"NUMEXPR_NUM_THREADS\"] = \"1\" # export NUMEXPR_NUM_THREADS=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "print(\"Num GPUs:\", len(physical_devices))\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(gpus[0], True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "  try:\n",
    "    tf.config.experimental.set_virtual_device_configuration(gpus[0], [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=5120)])\n",
    "  except RuntimeError as e:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\prati\\AppData\\Local\\Programs\\Python\\Python39\\python.exe\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(sys.executable)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 2.5e-4\n",
    "\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "EPSILON_MAX = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.98"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space=4\n",
    "obs_space=2\n",
    "action_space=2\n",
    "intermediate_dim=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "env=StatelessCartPole()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModel:\n",
    "    def __init__(self, input_dim, output_dim, lr):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        self.Qpolicy = self.create()\n",
    "        self.Qtarget = self.create() \n",
    "        self.Qtarget.set_weights(self.Qpolicy.get_weights())\n",
    "        \n",
    "    def create(self):\n",
    "        model = Sequential()\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=(1,state_space)))\n",
    "        model.add(Dense(512,activation = 'relu'))\n",
    "        model.add(Dense(256, activation = 'relu'))\n",
    "        model.add(Dense(128, activation = 'relu'))\n",
    "        model.add(Dense(self.output_dim, activation = 'linear'))\n",
    "        model.compile(optimizer = RMSprop(learning_rate = self.lr, rho = 0.95, epsilon = 1e-7), loss = \"mse\", metrics = ['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSolver:\n",
    "    def __init__(self, state_space,action_space, decay_coe = 0.99975, \n",
    "                  memory_size = 10_000,  C = 5):\n",
    "        \n",
    "        #self.env = gym.make('CartPole-v0')\n",
    "\n",
    "        self.states = state_space\n",
    "        self.n_actions = action_space\n",
    "        \n",
    "        self.actions = [i for i in range(self.n_actions)]\n",
    "        \n",
    "        self.lr = LEARNING_RATE\n",
    "        self.gamma = GAMMA\n",
    "        self.epsilon = EPSILON_MAX\n",
    "        self.decay_coe = decay_coe\n",
    "        self.min_eps = EPSILON_MIN\n",
    "        #self.episodes = episodes\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.memory = deque(maxlen = memory_size) # replay memory \n",
    "        self.C = C\n",
    "        \n",
    "        self.terminal_state = False # end of the episode\n",
    "        self.target_counter = 0 \n",
    "        \n",
    "        # Plot data\n",
    "        #self.timestep = self.episodes / 10\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.model = QModel((1,self.states), self.n_actions, self.lr)\n",
    "        # Smooth epsilon \n",
    "        # self.a = 0.35\n",
    "        # self.b = 0.1\n",
    "        # self.c = 0.01\n",
    "        \n",
    "    def state_shape(self,states):\n",
    "        states = np.array(states)\n",
    "        return states.reshape(-1,*states.shape)\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        Updates the current target_q_net with the q_net which brings all the\n",
    "        training in the q_net to the target_q_net.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.model.Qtarget.set_weights(self.model.Qpolicy.get_weights())\n",
    "    def decrement_epsilon(self):\n",
    "        '''\n",
    "        if self.epsilon > self.min_eps:\n",
    "            self.epsilon *= self.decay_coe\n",
    "        else:\n",
    "            self.epsilon = self.min_eps\n",
    "        '''\n",
    "        # s_time = (time - self.a*self.episodes) / (self.b*self.episodes) \n",
    "        # cosh = np.cosh(math.exp(-s_time))\n",
    "        # self.epsilon = 1 - (1/cosh + (time*self.c/self.episodes))\n",
    "        if self.epsilon>self.min_eps:\n",
    "            self.epsilon*=EXPLORATION_DECAY\n",
    "        else:\n",
    "            self.epsilon=self.min_eps\n",
    "    def forget(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    def remember(self, s, a, r, s_, done):\n",
    "        self.memory.append([self.state_shape(s), a, r, self.state_shape(s_), done])\n",
    "        \n",
    "    def act(self, states):\n",
    "        if np.random.random() > (1 - self.epsilon):\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            states = self.state_shape(states)\n",
    "            states.reshape(1,1,self.states)\n",
    "#             states=[states]\n",
    "#             states=np.array(states)\n",
    "            #print(states.shape)\n",
    "            action = np.argmax(np.array(self.model.Qpolicy.predict_on_batch(states)))\n",
    "            \n",
    "        return action\n",
    "            \n",
    "    def minibatch(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "    \n",
    "\n",
    "        #plt.savefig(r'RL/loss - e{}v2.png'.format(episode), dpi = 500)\n",
    "        \n",
    "    def train(self):\n",
    "        # X - states passed to the NN, y - target\n",
    "        \n",
    "        X, y = [], []\n",
    "        \n",
    "        if len(self.memory) >= self.batch_size: \n",
    "            SARS = self.minibatch()\n",
    "        \n",
    "            s = self.state_shape([row[0] for row in SARS])\n",
    "            s=s.reshape(BATCH_SIZE,1,state_space)\n",
    "            #print(s.shape)\n",
    "            qvalue = np.array(self.model.Qpolicy.predict_on_batch(s))\n",
    "            #print(qvalue)\n",
    "\n",
    "            s_ = self.state_shape([row[3] for row in SARS])\n",
    "            s_=s_.reshape(BATCH_SIZE,1,state_space)\n",
    "            future_qvalue = np.array(self.model.Qtarget.predict_on_batch(s_))\n",
    "            #print(\"2\")\n",
    "            #print(future_qvalue)\n",
    "\n",
    "            for index, (state, action, reward, state_, done) in enumerate(SARS):\n",
    "                if done == True:\n",
    "                    Qtarget = reward\n",
    "                else:\n",
    "                    Qtarget = reward + self.gamma * np.max(future_qvalue[index][0])\n",
    "            \n",
    "                qcurr = qvalue[index][0]\n",
    "                #print(qcurr)\n",
    "                qcurr[int(action)] = Qtarget \n",
    "                #print(qcurr)\n",
    "                X.append(state)\n",
    "                y.append(qcurr)\n",
    "#             X_dataset=tf.data.Dataset.from_tensor_slices(X).batch(64)\n",
    "#             y_dataset=tf.data.Dataset.from_tensor_slices(y).batch(64)\n",
    "#             final_dataset=tf.data.Dataset.zip((X_dataset, y_dataset))\n",
    "            X, y = np.array(X).reshape(self.batch_size,1,self.states), np.array(y).reshape(self.batch_size, 1, self.n_actions)\n",
    "            \n",
    "           #print(X.shape,\"   \",y.shape  )                          \n",
    "            #loss = self.model.Qpolicy.fit(final_dataset,verbose=0)   \n",
    "            X, y = np.array(X).reshape(self.batch_size,1,self.states), np.array(y).reshape(self.batch_size, 1, self.n_actions)\n",
    "           # print(self.model.Qpolicy.predict_on_batch(X))\n",
    "           #print(X.shape,\"   \",y.shape  )                          \n",
    "            loss = self.model.Qpolicy.train_on_batch(X, y,return_dict=True)\n",
    "            \n",
    "            #self.history.append(loss.history['loss'][0])\n",
    "            \n",
    "                \n",
    "            # if self.terminal_state:\n",
    "            #     self.target_counter+=1\n",
    "\n",
    "            # # C -> target network update frequency\n",
    "            # if self.target_counter > self.C: \n",
    "            #     self.model.Qtarget.set_weights(self.model.Qpolicy.get_weights())\n",
    "            #     self.target_counter = 0 \n",
    "\n",
    "                \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "  def __init__(self,state_space, intermediate_dim):\n",
    "    super(Encoder, self).__init__()\n",
    "    self.input_layer=tf.keras.layers.InputLayer(input_shape=(7,))\n",
    "    self.hidden_layer1=tf.keras.layers.Dense(\n",
    "      units=intermediate_dim,\n",
    "      activation=tf.nn.relu,\n",
    "      kernel_initializer='he_uniform'\n",
    "    )\n",
    "    #self.batchnorm_layer1=tf.keras.layers.BatchNormalization()\n",
    "    self.hidden_layer2 = tf.keras.layers.Dense(\n",
    "      units=intermediate_dim,\n",
    "      activation=tf.nn.relu,\n",
    "      kernel_initializer='he_uniform'\n",
    "    )\n",
    "    #self.batchnorm_layer2=tf.keras.layers.BatchNormalization()\n",
    "    self.output_layer = tf.keras.layers.Dense(\n",
    "      units=state_space,\n",
    "      activation=tf.keras.activations.linear\n",
    "    )\n",
    "    \n",
    "  def call(self, input_features):\n",
    "    activation0=self.input_layer(input_features)\n",
    "    activation1 = self.hidden_layer1(activation0)\n",
    "    #activation1=self.batchnorm_layer1(activation0)\n",
    "    activation2=self.hidden_layer2(activation1)\n",
    "    #activation2=self.batchnorm_layer2(activation1)\n",
    "    return self.output_layer(activation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_solver=DQNSolver(state_space=state_space,action_space=action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "  def __init__(self, intermediate_dim, obs_space):\n",
    "    super().__init__()\n",
    "    self.hidden_layer1 = tf.keras.layers.Dense(\n",
    "      units=intermediate_dim,\n",
    "      activation=tf.nn.relu,\n",
    "      kernel_initializer='he_uniform'\n",
    "    )\n",
    "    #self.batchnorm_layer1=tf.keras.layers.BatchNormalization()\n",
    "    self.hidden_layer2 = tf.keras.layers.Dense(\n",
    "      units=intermediate_dim,\n",
    "      activation=tf.nn.relu,\n",
    "      kernel_initializer='he_uniform'\n",
    "    )\n",
    "    #self.batchnorm_layer2=tf.keras.layers.BatchNormalization()\n",
    "    self.output_layer = tf.keras.layers.Dense(\n",
    "      units=obs_space,\n",
    "      activation=tf.keras.activations.linear\n",
    "    )\n",
    "  \n",
    "  def call(self, input_features,var):\n",
    "    if(len(input_features.get_shape())==2):\n",
    "      activation1 = self.hidden_layer1(tf.concat((input_features,var),axis=1))\n",
    "    else:\n",
    "      temp=[]\n",
    "      for i in range(len(input_features)):\n",
    "          temp.append(tf.concat((input_features[i],var[i]),axis=1))\n",
    "      temp=tf.convert_to_tensor(temp)\n",
    "      activation1 = self.hidden_layer1(temp)\n",
    "    #activation2=self.batchnorm_layer1(activation1)\n",
    "    activation2=self.hidden_layer2(activation1)\n",
    "    #activation4=self.batchnorm_layer2(activation3)\n",
    "\n",
    "    return self.output_layer(activation2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(tf.keras.Model):\n",
    "  def __init__(self, intermediate_dim, state_space,obs_space):\n",
    "    super(Autoencoder, self).__init__()\n",
    "    self.encoder = Encoder(intermediate_dim=intermediate_dim,state_space=state_space)\n",
    "    \n",
    "    self.decoder = Decoder(\n",
    "      intermediate_dim=intermediate_dim,\n",
    "      obs_space=obs_space\n",
    "    )\n",
    "    \n",
    "  \n",
    "  def call(self, input_features,var):\n",
    "    code = self.encoder(input_features)\n",
    "    #self.var=tf.Variable(var,trainable=False)\n",
    "    reconstructed = self.decoder(code,var)\n",
    "    return reconstructed\n",
    "\n",
    "\n",
    "opt = tf.optimizers.Adam(learning_rate=LEARNING_RATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def loss(model,input_features, obs,var):\n",
    "  reconstruction_error = tf.reduce_mean(tf.square(tf.subtract(model(input_features,var), obs)))\n",
    "  return reconstruction_error\n",
    "  \n",
    "def train(loss, model, opt,input_features,obs,var):\n",
    "  with tf.GradientTape() as tape:\n",
    "    gradients = tape.gradient(loss(model,input_features, obs,var), model.trainable_variables)\n",
    "  gradient_variables = zip(gradients, model.trainable_variables)\n",
    "  opt.apply_gradients(gradient_variables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.04179921,  0.04053216,  0.01114889, -0.00987563], dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs=env.reset()\n",
    "obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "autoencoder = Autoencoder(\n",
    "  intermediate_dim=intermediate_dim,\n",
    "  state_space=state_space,obs_space=obs_space\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_collection(total_episodes,dqn_solver,autoencoder):\n",
    "\n",
    "    S_series=[]\n",
    "    S_actual_series=[]\n",
    "    O_series=[]\n",
    "    U_series=[]\n",
    "    R_series=[]\n",
    "    O_predicted_series=[]\n",
    "    Done_series=[]\n",
    "    O_series.append(np.array([[0.0,0.0]]))\n",
    "    O_predicted_series.append(np.array([[0.0,0.0]]))\n",
    "    encoder=autoencoder.encoder\n",
    "    decoder=autoencoder.decoder\n",
    "\n",
    "    ep_no=0\n",
    "    i=0\n",
    "    while(ep_no<total_episodes):\n",
    "        s0=env.reset()\n",
    "        #s0=np.reshape(s0,[1,state_space])\n",
    "        \n",
    "        step=0\n",
    "        done=False\n",
    "        while not done:\n",
    "            if(step==0):\n",
    "                s=s0\n",
    "                S_actual_series.append(s)\n",
    "            else:\n",
    "                s=encoder(tf.concat((S_series[i-1],O_series[i],U_series[i-1]),axis=1))\n",
    "                s=np.reshape(s,(1,state_space))\n",
    "                # s=s.tolist()\n",
    "                # s[0][0]=np.tanh(s[0][0])*4.8\n",
    "                # s[0][2]=np.tanh(s[0][2])*0.418\n",
    "                # s=np.array(s)\n",
    "                \n",
    "\n",
    "            s=np.reshape(s,(1,state_space))\n",
    "            S_series.append(s)\n",
    "            action=dqn_solver.act(s)\n",
    "\n",
    "            obs, reward, done, info = env.step(int(action))\n",
    "            \n",
    "            Done_series.append(done)\n",
    "            #actual_state=np.reshape(actual_state,[1,state_space])\n",
    "            #S_actual_series.append(actual_state)\n",
    "            action=np.array([[action]])\n",
    "            action=action.astype('float32')\n",
    "            U_series.append(action)\n",
    "            obs=np.reshape(obs,[1,obs_space])\n",
    "\n",
    "            O_series.append(obs)\n",
    "            \n",
    "            R_series.append(reward)\n",
    "            # print(s.shape)\n",
    "            # print(action.shape)\n",
    "            obs_pred=decoder(s,action)\n",
    "            obs_pred=np.reshape(obs_pred,[1,obs_space])\n",
    "            obs_pred=obs_pred.tolist()\n",
    "            # obs_pred[0][0]=np.tanh(obs_pred[0][0])*4.8\n",
    "            # obs_pred[0][1]=np.tanh(obs_pred[0][1])*0.418\n",
    "            obs_pred=np.array(obs_pred)\n",
    "            O_predicted_series.append(obs_pred)\n",
    "            i+=1\n",
    "            step+=1\n",
    "        ep_no+=1\n",
    "    return S_series,O_series,U_series,R_series,O_predicted_series,Done_series,S_actual_series\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dqn_training(dqn_solver,epochs,S_series,U_series,R_series,Done_series):\n",
    "    dqn_solver.forget()\n",
    "    for i in range(len(S_series)-1):\n",
    "        dqn_solver.remember(S_series[i][0],U_series[i][0][0],R_series[i],S_series[i+1][0],Done_series[i])\n",
    "    for j in trange(epochs):\n",
    "        dqn_solver.train()\n",
    "        if(j%5==0):\n",
    "            dqn_solver.update_target_model()\n",
    "        time.sleep(0.002)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_training(epochs,autoencoder,S_series,O_series,U_series):\n",
    "    temp1=[]\n",
    "    temp2=[]\n",
    "    temp3=[]\n",
    "    for i in range(2,len(S_series)-1):\n",
    "        temp1.append(tf.concat((S_series[i-1],O_series[i],U_series[i-1]),axis=1))\n",
    "        temp2.append(O_series[i+1])\n",
    "        temp3.append(U_series[i])\n",
    "    input_features_dataset=tf.data.Dataset.from_tensor_slices(temp1).batch(32)\n",
    "    O_actual_dataset=tf.data.Dataset.from_tensor_slices(temp2).batch(32)\n",
    "    var_dataset=tf.data.Dataset.from_tensor_slices(temp3).batch(32)\n",
    "    final_dataset=tf.data.Dataset.zip((input_features_dataset, O_actual_dataset,var_dataset))\n",
    "    \n",
    "    for epoch in trange(epochs):\n",
    "        for x in final_dataset:\n",
    "            train(loss,autoencoder,opt,x[0],x[1],x[2])\n",
    "        time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "rewards_storage=[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#S_series,O_series,U_series,R_series,O_predicted_series,S_actual_series,Done_series=data_collection(100,dqn_solver,autoencoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env.step(int(U_series[2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#O_series[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_loop(autoencoder,dqn_solver,epochs):\n",
    "    for epoch in range(epochs):\n",
    "        S_series,O_series,U_series,R_series,O_predicted_series,Done_series=data_collection(100,dqn_solver,autoencoder)\n",
    "        print(\"epoch: {}, total reward: {}\".format(epoch, sum(R_series)))\n",
    "        rewards_storage.append(sum(R_series))\n",
    "        if(sum(R_series)>20000):\n",
    "            df=pd.DataFrame(rewards_storage)\n",
    "            df.to_csv('rewards_final_main_1.csv')\n",
    "            \n",
    "        if(epoch<200):\n",
    "            if(epoch%2==0):\n",
    "                dqn_training(dqn_solver,500,S_series,U_series,R_series,Done_series)\n",
    "                dqn_solver.decrement_epsilon()\n",
    "                \n",
    "            else:\n",
    "                autoencoder_training(25,autoencoder,S_series,O_series,U_series)\n",
    "        else:\n",
    "            dqn_training(dqn_solver,500,S_series,U_series,R_series,Done_series)\n",
    "            dqn_solver.decrement_epsilon()\n",
    "        del S_series\n",
    "        del O_series\n",
    "        del U_series\n",
    "        del R_series\n",
    "        del O_predicted_series\n",
    "        del S_actual_series\n",
    "        del Done_series\n",
    "        gc.collect()\n",
    "        #time.sleep(0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, total reward: 2119.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 183/500 [00:11<00:19, 16.06it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22260/594783145.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcomplete_loop\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mautoencoder\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mdqn_solver\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1001\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22260/3153301291.py\u001b[0m in \u001b[0;36mcomplete_loop\u001b[1;34m(autoencoder, dqn_solver, epochs)\u001b[0m\n\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m<\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 12\u001b[1;33m                 \u001b[0mdqn_training\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdqn_solver\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m500\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mS_series\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mU_series\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mR_series\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDone_series\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     13\u001b[0m                 \u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecrement_epsilon\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22260/1319579334.py\u001b[0m in \u001b[0;36mdqn_training\u001b[1;34m(dqn_solver, epochs, S_series, U_series, R_series, Done_series)\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremember\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mS_series\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mU_series\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mR_series\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mS_series\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mDone_series\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mj\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m%\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[0mdqn_solver\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate_target_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22260/670580147.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    126\u001b[0m            \u001b[1;31m# print(self.model.Qpolicy.predict_on_batch(X))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m            \u001b[1;31m#print(X.shape,\"   \",y.shape  )\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mQpolicy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mreturn_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[1;31m#self.history.append(loss.history['loss'][0])\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\prati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1826\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1827\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1828\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1829\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1830\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\prati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1765\u001b[0m     \"\"\"\n\u001b[0;32m   1766\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1767\u001b[1;33m       \u001b[0mm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_state\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1768\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1769\u001b[0m   def train_on_batch(self,\n",
      "\u001b[1;32mc:\\Users\\prati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\metrics.py\u001b[0m in \u001b[0;36mreset_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    258\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    259\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 260\u001b[1;33m       \u001b[0mbackend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    261\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\prati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\util\\dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    204\u001b[0m     \u001b[1;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 206\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    207\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    208\u001b[0m       \u001b[1;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\prati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   3802\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3803\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3804\u001b[1;33m       \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0massign\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdtype_numpy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3805\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3806\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\prati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[1;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[0;32m    900\u001b[0m              \"shape %s are incompatible\") %\n\u001b[0;32m    901\u001b[0m             (tensor_name, self._shape, value_tensor.shape))\n\u001b[1;32m--> 902\u001b[1;33m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[0m\u001b[0;32m    903\u001b[0m           self.handle, value_tensor, name=name)\n\u001b[0;32m    904\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\prati\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tensorflow\\python\\ops\\gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[1;34m(resource, value, name)\u001b[0m\n\u001b[0;32m    138\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mtld\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_eager\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0m\u001b[0;32m    141\u001b[0m         _ctx, \"AssignVariableOp\", name, resource, value)\n\u001b[0;32m    142\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "complete_loop(autoencoder,dqn_solver,1001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=tf.concat((S_series[3-1],O_series[3],U_series[3-1]),axis=1)\n",
    "# print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(loss(autoencoder,x,O_series[4],U_series[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.plot(rewards_storage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i,val in enumerate(rewards_storage):\n",
    "    if (val>10000):\n",
    "        print(\"{} , {}\".format(i,val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y=rewards_storage\n",
    "# x=[float(i) for i in range (1,len(rewards_storage)+1)]\n",
    "# x=np.array(x)\n",
    "# m,b=np.polyfit(x,y,1)\n",
    "# plt.plot(rewards_storage)\n",
    "# plt.plot(x,m*x+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(rewards_storage)\n",
    "df.to_csv('rewards_final_main.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_avg=df.rolling(window=10).mean()\n",
    "# plt.plot(df_avg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "0ef4eba0bef179de2f7a9543a17afda401074cec081dd552282a874e9ff31d3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
