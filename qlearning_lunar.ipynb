{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.activations import relu, linear\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import math\n",
    "from random import sample\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from lunar_lander import LunarLander\n",
    "env = LunarLander('LunarLander-v2',\n",
    "    continuous= False,\n",
    "    gravity = -10.0,\n",
    "    enable_wind = True,\n",
    "    wind_power = 5.0,\n",
    "    turbulence_power= 0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.action_space.n\n",
    "env.observation_space.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "GAMMA = 0.99\n",
    "LEARNING_RATE = 2.5e-4\n",
    "\n",
    "MEMORY_SIZE = 1000000\n",
    "BATCH_SIZE = 64\n",
    "\n",
    "EPSILON_MAX = 1.0\n",
    "EPSILON_MIN = 0.01\n",
    "EXPLORATION_DECAY = 0.995"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space=8\n",
    "obs_space=8\n",
    "action_space=4\n",
    "intermediate_dim=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModel:\n",
    "    def __init__(self, input_dim, output_dim, lr):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        self.Qpolicy = self.create()\n",
    "        self.Qtarget = self.create() \n",
    "        self.Qtarget.set_weights(self.Qpolicy.get_weights())\n",
    "        \n",
    "    def create(self):\n",
    "        model = Sequential()\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=(1,self.input_dim)))\n",
    "        model.add(Dense(512,activation = 'relu',kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(256, activation = 'relu',kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(128, activation = 'relu',kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.output_dim, activation = 'linear',kernel_initializer='he_uniform'))\n",
    "        model.compile(optimizer = RMSprop(learning_rate = self.lr, rho = 0.95, epsilon = 1e-7), loss = \"mse\", metrics = ['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSolver:\n",
    "    def __init__(self, state_space,action_space, decay_coe = 0.97, \n",
    "                  memory_size = 10000,  C = 5,LEARNING_RATE=2.5e-5,GAMMA=1,EPSILON_MAX=1.0,EPSILON_MIN=0.01,BATCH_SIZE=64):\n",
    "        \n",
    "        #self.env = gym.make('CartPole-v0')\n",
    "\n",
    "        self.states = state_space\n",
    "        self.n_actions = action_space\n",
    "        \n",
    "        self.actions = [i for i in range(self.n_actions)]\n",
    "        \n",
    "        self.lr = LEARNING_RATE\n",
    "        self.gamma = GAMMA\n",
    "        self.epsilon = EPSILON_MAX\n",
    "        self.decay_coe = decay_coe\n",
    "        self.min_eps = EPSILON_MIN\n",
    "        #self.episodes = episodes\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.memory = deque(maxlen = memory_size) # replay memory \n",
    "        self.C = C\n",
    "        \n",
    "        self.terminal_state = False # end of the episode\n",
    "        self.target_counter = 0 \n",
    "        \n",
    "        # Plot data\n",
    "        #self.timestep = self.episodes / 10\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.model = QModel(self.states, self.n_actions, self.lr)\n",
    "        self.positive_rewards_list=[]\n",
    "        # Smooth epsilon \n",
    "        # self.a = 0.35\n",
    "        # self.b = 0.1\n",
    "        # self.c = 0.01\n",
    "    def find_positive_rewards(self):\n",
    "        self.positive_rewards_list=[]\n",
    "        for i,x in enumerate(self.memory):\n",
    "            if x[2]>10:\n",
    "                self.positive_rewards_list.append(i)\n",
    "    def state_shape(self,states):\n",
    "        states = np.array(states)\n",
    "        return states.reshape(-1,*states.shape)\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        Updates the current target_q_net with the q_net which brings all the\n",
    "        training in the q_net to the target_q_net.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.model.Qtarget.set_weights(self.model.Qpolicy.get_weights())\n",
    "    def decrement_epsilon(self):\n",
    "        '''\n",
    "        if self.epsilon > self.min_eps:\n",
    "            self.epsilon *= self.decay_coe\n",
    "        else:\n",
    "            self.epsilon = self.min_eps\n",
    "        '''\n",
    "        # s_time = (time - self.a*self.episodes) / (self.b*self.episodes) \n",
    "        # cosh = np.cosh(math.exp(-s_time))\n",
    "        # self.epsilon = 1 - (1/cosh + (time*self.c/self.episodes))\n",
    "        if self.epsilon>self.min_eps:\n",
    "            self.epsilon*=self.decay_coe\n",
    "        else:\n",
    "            self.epsilon=self.min_eps\n",
    "    def forget(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    def remember(self, s, a, r, s_, done):\n",
    "        self.memory.append([self.state_shape(s), a, r, self.state_shape(s_), done])\n",
    "        \n",
    "    def act(self, states):\n",
    "        if np.random.random() > (1 - self.epsilon):\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            states = self.state_shape(states)\n",
    "            states.reshape(1,1,self.states)\n",
    "#             states=[states]\n",
    "#             states=np.array(states)\n",
    "            #print(states.shape)\n",
    "            action = np.argmax(np.array(self.model.Qpolicy.predict_on_batch(states)))\n",
    "            \n",
    "        return action\n",
    "            \n",
    "    def minibatch(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "        # indices=[]\n",
    "        # minibatch=[]\n",
    "        # if(len(self.positive_rewards_list)>10):\n",
    "        #     indices=random.sample(self.positive_rewards_list,10)\n",
    "        #     for i in indices:\n",
    "        #         minibatch.append(self.memory[i])\n",
    "        #     minibatch=minibatch+random.sample(self.memory, self.batch_size-10)\n",
    "        #     random.shuffle(minibatch)            \n",
    "        #     return minibatch\n",
    "        # else:\n",
    "        #     for i in self.positive_rewards_list:\n",
    "        #         minibatch.append(self.memory[i])\n",
    "        #     minibatch=minibatch+random.sample(self.memory, self.batch_size-len(self.positive_rewards_list))\n",
    "        #     random.shuffle(minibatch)            \n",
    "        #     return minibatch\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        #plt.savefig(r'RL/loss - e{}v2.png'.format(episode), dpi = 500)\n",
    "        \n",
    "    def train(self):\n",
    "        # X - states passed to the NN, y - target\n",
    "        \n",
    "        X, y = [], []\n",
    "        \n",
    "        if len(self.memory) >= self.batch_size: \n",
    "            SARS = self.minibatch()\n",
    "        \n",
    "            s = self.state_shape([row[0] for row in SARS])\n",
    "            s=s.reshape(BATCH_SIZE,1,state_space)\n",
    "            #print(s.shape)\n",
    "            qvalue = np.array(self.model.Qpolicy.predict_on_batch(s))\n",
    "            #print(qvalue)\n",
    "\n",
    "            s_ = self.state_shape([row[3] for row in SARS])\n",
    "            s_=s_.reshape(BATCH_SIZE,1,state_space)\n",
    "            future_qvalue = np.array(self.model.Qtarget.predict_on_batch(s_))\n",
    "            #print(\"2\")\n",
    "            #print(future_qvalue)\n",
    "\n",
    "            for index, (state, action, reward, state_, done) in enumerate(SARS):\n",
    "                if done == True:\n",
    "                    Qtarget = reward\n",
    "                else:\n",
    "                    Qtarget = reward + self.gamma * np.max(future_qvalue[index][0])\n",
    "            \n",
    "                qcurr = qvalue[index][0]\n",
    "                #print(qcurr)\n",
    "                qcurr[int(action)] = Qtarget \n",
    "                #print(qcurr)\n",
    "                X.append(state)\n",
    "                y.append(qcurr)\n",
    "#             X_dataset=tf.data.Dataset.from_tensor_slices(X).batch(64)\n",
    "#             y_dataset=tf.data.Dataset.from_tensor_slices(y).batch(64)\n",
    "#             final_dataset=tf.data.Dataset.zip((X_dataset, y_dataset))\n",
    "            #X, y = np.array(X).reshape(self.batch_size,1,self.states), np.array(y).reshape(self.batch_size, 1, self.n_actions)\n",
    "            \n",
    "           #print(X.shape,\"   \",y.shape  )                          \n",
    "            #loss = self.model.Qpolicy.fit(final_dataset,verbose=0)   \n",
    "            X, y = np.array(X).reshape(self.batch_size,1,self.states), np.array(y).reshape(self.batch_size, 1, self.n_actions)\n",
    "           # print(self.model.Qpolicy.predict_on_batch(X))\n",
    "           #print(X.shape,\"   \",y.shape  )                          \n",
    "            self.model.Qpolicy.train_on_batch(X, y,return_dict=True)\n",
    "            \n",
    "            #self.history.append(loss.history['loss'][0])\n",
    "            \n",
    "                \n",
    "            # if self.terminal_state:\n",
    "            #     self.target_counter+=1\n",
    "\n",
    "            # # C -> target network update frequency\n",
    "            # if self.target_counter > self.C: \n",
    "            #     self.model.Qtarget.set_weights(self.model.Qpolicy.get_weights())\n",
    "            #     self.target_counter = 0 \n",
    "\n",
    "                \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_solver=DQNSolver(state_space=state_space,action_space=action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "no_train=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=env.reset()\n",
    "s=s[0]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s=[s[0],s[1],s[4],s[6],s[7]]\n",
    "print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dqn_solver.memory[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#qn_solver.state_shape(dqn_solver.memory[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(dqn_solver,n_episodes=4000,maxt=1000):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "     # last 100 scores\n",
    "    trains=0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        state=state[0]\n",
    "        score = 0\n",
    "\n",
    "        done=False\n",
    "\n",
    "        for t in tqdm(range(maxt),leave=False,desc=str(i_episode)):\n",
    "            action = dqn_solver.act(state)\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            \n",
    "            dqn_solver.remember(state,action,reward,next_state,done)\n",
    "\n",
    "            dqn_solver.train()\n",
    "            trains+=1\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        dqn_solver.decrement_epsilon()\n",
    "        scores.append(score)\n",
    "        no_train.append(trains)             # save most recent score\n",
    "        if(i_episode%5==0):\n",
    "            dqn_solver.update_target_model()\n",
    "        if(i_episode%20==0):\n",
    "            print('\\rEpisode {}\\tScore: {:.2f}'.format(i_episode, score, end=\"\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training(dqn_solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(scores)\n",
    "df.columns=['scores']\n",
    "df.to_csv('rewards_only_dqn_normal_lunar_3.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(scores[:1000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(no_train,scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(no_train[0:40])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls=[]\n",
    "for x in scores:\n",
    "    if x>0:\n",
    "        ls.append(x)\n",
    "    else:\n",
    "        ls.append(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['positive']=ls\n",
    "df['moving_avg']=df.scores.rolling(100,min_periods=1).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize']=[20,10]\n",
    "\n",
    "plt.plot(df['moving_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(no_train,df['moving_avg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0ef4eba0bef179de2f7a9543a17afda401074cec081dd552282a874e9ff31d3f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
