{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from collections import deque\n",
    "import tensorflow as tf\n",
    "from tqdm import tqdm \n",
    "from tensorflow.keras import Sequential \n",
    "from tensorflow.keras.activations import relu, linear\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "import math\n",
    "from random import sample\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tqdm import trange\n",
    "import pandas as pd\n",
    "import gc\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCar-v0')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_space=2\n",
    "obs_space=2\n",
    "action_space=3\n",
    "intermediate_dim=16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QModel:\n",
    "    def __init__(self, input_dim, output_dim, lr):\n",
    "        self.input_dim = input_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.lr = lr\n",
    "        self.Qpolicy = self.create()\n",
    "        self.Qtarget = self.create() \n",
    "        self.Qtarget.set_weights(self.Qpolicy.get_weights())\n",
    "        \n",
    "    def create(self):\n",
    "        model = Sequential()\n",
    "        model.add(tf.keras.layers.InputLayer(input_shape=(1,self.input_dim)))\n",
    "        model.add(Dense(512,activation = 'relu',kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(256, activation = 'relu',kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(128, activation = 'relu',kernel_initializer='he_uniform'))\n",
    "        model.add(Dense(self.output_dim, activation = 'linear',kernel_initializer='he_uniform'))\n",
    "        model.compile(optimizer = RMSprop(learning_rate = self.lr, rho = 0.95, epsilon = 1e-7), loss = \"mse\", metrics = ['accuracy'])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE=64\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNSolver:\n",
    "    def __init__(self, state_space,action_space, decay_coe = 0.99, \n",
    "                  memory_size = 20000,  C = 5,LEARNING_RATE=1e-4,GAMMA=0.99,EPSILON_MAX=1.0,EPSILON_MIN=0.01,BATCH_SIZE=64):\n",
    "        \n",
    "        #self.env = gym.make('CartPole-v0')\n",
    "        self.states = state_space\n",
    "        self.n_actions = action_space\n",
    "        \n",
    "        self.actions = [i for i in range(self.n_actions)]\n",
    "        \n",
    "        self.lr = LEARNING_RATE\n",
    "        self.gamma = GAMMA\n",
    "        self.epsilon = EPSILON_MAX\n",
    "        self.decay_coe = decay_coe\n",
    "        self.min_eps = EPSILON_MIN\n",
    "        #self.episodes = episodes\n",
    "        self.batch_size = BATCH_SIZE\n",
    "        self.memory = deque(maxlen = memory_size) # replay memory \n",
    "        self.C = C\n",
    "        \n",
    "        self.terminal_state = False # end of the episode\n",
    "        self.target_counter = 0 \n",
    "        \n",
    "        # Plot data\n",
    "        #self.timestep = self.episodes / 10\n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        self.model = QModel(self.states, self.n_actions, self.lr)\n",
    "        self.positive_rewards_list=[]\n",
    "        # Smooth epsilon \n",
    "        # self.a = 0.35\n",
    "        # self.b = 0.1\n",
    "        # self.c = 0.01\n",
    "    def find_positive_rewards(self):\n",
    "        self.positive_rewards_list=[]\n",
    "        for i,x in enumerate(self.memory):\n",
    "            if x[2]>10:\n",
    "                self.positive_rewards_list.append(i)\n",
    "    def state_shape(self,states):\n",
    "        states = np.array(states)\n",
    "        return states.reshape(-1,*states.shape)\n",
    "    def update_target_model(self):\n",
    "        \"\"\"\n",
    "        Updates the current target_q_net with the q_net which brings all the\n",
    "        training in the q_net to the target_q_net.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.model.Qtarget.set_weights(self.model.Qpolicy.get_weights())\n",
    "    def decrement_epsilon(self):\n",
    "        '''\n",
    "        if self.epsilon > self.min_eps:\n",
    "            self.epsilon *= self.decay_coe\n",
    "        else:\n",
    "            self.epsilon = self.min_eps\n",
    "        '''\n",
    "        # s_time = (time - self.a*self.episodes) / (self.b*self.episodes) \n",
    "        # cosh = np.cosh(math.exp(-s_time))\n",
    "        # self.epsilon = 1 - (1/cosh + (time*self.c/self.episodes))\n",
    "        if self.epsilon>self.min_eps:\n",
    "            self.epsilon*=self.decay_coe\n",
    "        else:\n",
    "            self.epsilon=self.min_eps\n",
    "    def forget(self):\n",
    "        self.memory.clear()\n",
    "\n",
    "    def remember(self, s, a, r, s_, done):\n",
    "        self.memory.append([self.state_shape(s), a, r, self.state_shape(s_), done])\n",
    "        \n",
    "    def act(self, states):\n",
    "        if np.random.random() > (1 - self.epsilon):\n",
    "            action = np.random.choice(self.actions)\n",
    "        else:\n",
    "            states = self.state_shape(states)\n",
    "            states.reshape(1,1,self.states)\n",
    "#             states=[states]\n",
    "#             states=np.array(states)\n",
    "            #print(states.shape)\n",
    "            action = np.argmax(np.array(self.model.Qpolicy.predict_on_batch(states)))\n",
    "            \n",
    "        return action\n",
    "            \n",
    "    def minibatch(self):\n",
    "        return random.sample(self.memory, self.batch_size)\n",
    "        # indices=[]\n",
    "        # minibatch=[]\n",
    "        # if(len(self.positive_rewards_list)>10):\n",
    "        #     indices=random.sample(self.positive_rewards_list,10)\n",
    "        #     for i in indices:\n",
    "        #         minibatch.append(self.memory[i])\n",
    "        #     minibatch=minibatch+random.sample(self.memory, self.batch_size-10)\n",
    "        #     random.shuffle(minibatch)            \n",
    "        #     return minibatch\n",
    "        # else:\n",
    "        #     for i in self.positive_rewards_list:\n",
    "        #         minibatch.append(self.memory[i])\n",
    "        #     minibatch=minibatch+random.sample(self.memory, self.batch_size-len(self.positive_rewards_list))\n",
    "        #     random.shuffle(minibatch)            \n",
    "        #     return minibatch\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "        #plt.savefig(r'RL/loss - e{}v2.png'.format(episode), dpi = 500)\n",
    "        \n",
    "    def train(self):\n",
    "        # X - states passed to the NN, y - target\n",
    "        \n",
    "        X, y = [], []\n",
    "        \n",
    "        if len(self.memory) >= self.batch_size: \n",
    "            SARS = self.minibatch()\n",
    "        \n",
    "            s = self.state_shape([row[0] for row in SARS])\n",
    "            s=s.reshape(BATCH_SIZE,1,state_space)\n",
    "            #print(s.shape)\n",
    "            qvalue = np.array(self.model.Qpolicy.predict_on_batch(s))\n",
    "            #print(qvalue)\n",
    "\n",
    "            s_ = self.state_shape([row[3] for row in SARS])\n",
    "            s_=s_.reshape(BATCH_SIZE,1,state_space)\n",
    "            future_qvalue = np.array(self.model.Qtarget.predict_on_batch(s_))\n",
    "            #print(\"2\")\n",
    "            #print(future_qvalue)\n",
    "\n",
    "            for index, (state, action, reward, state_, done) in enumerate(SARS):\n",
    "                if done == True:\n",
    "                    Qtarget = reward\n",
    "                else:\n",
    "                    Qtarget = reward + self.gamma * np.max(future_qvalue[index][0])\n",
    "            \n",
    "                qcurr = qvalue[index][0]\n",
    "                #print(qcurr)\n",
    "                qcurr[int(action)] = Qtarget \n",
    "                #print(qcurr)\n",
    "                X.append(state)\n",
    "                y.append(qcurr)\n",
    "#             X_dataset=tf.data.Dataset.from_tensor_slices(X).batch(64)\n",
    "#             y_dataset=tf.data.Dataset.from_tensor_slices(y).batch(64)\n",
    "#             final_dataset=tf.data.Dataset.zip((X_dataset, y_dataset))\n",
    "            #X, y = np.array(X).reshape(self.batch_size,1,self.states), np.array(y).reshape(self.batch_size, 1, self.n_actions)\n",
    "            \n",
    "           #print(X.shape,\"   \",y.shape  )                          \n",
    "            #loss = self.model.Qpolicy.fit(final_dataset,verbose=0)   \n",
    "            X, y = np.array(X).reshape(self.batch_size,1,self.states), np.array(y).reshape(self.batch_size, 1, self.n_actions)\n",
    "           # print(self.model.Qpolicy.predict_on_batch(X))\n",
    "           #print(X.shape,\"   \",y.shape  )                          \n",
    "            self.model.Qpolicy.train_on_batch(X, y,return_dict=True)\n",
    "            \n",
    "            #self.history.append(loss.history['loss'][0])\n",
    "            \n",
    "                \n",
    "            # if self.terminal_state:\n",
    "            #     self.target_counter+=1\n",
    "\n",
    "            # # C -> target network update frequency\n",
    "            # if self.target_counter > self.C: \n",
    "            #     self.model.Qtarget.set_weights(self.model.Qpolicy.get_weights())\n",
    "            #     self.target_counter = 0 \n",
    "\n",
    "                \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dqn_solver=DQNSolver(state_space=state_space,action_space=action_space)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores=[]\n",
    "no_train=[]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5979995,  0.       ], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s=env.reset()\n",
    "s[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(dqn_solver,n_episodes=4000,maxt=200):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "     # last 100 scores\n",
    "    trains=0\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        state=state[0]\n",
    "        score = 0\n",
    "\n",
    "        done=False\n",
    "\n",
    "        for t in tqdm(range(maxt),leave=False,desc=str(i_episode)):\n",
    "            action = dqn_solver.act(state)\n",
    "            next_state, reward, done, _,_ = env.step(action)\n",
    "            \n",
    "            dqn_solver.remember(state,action,reward,next_state,done)\n",
    "\n",
    "            dqn_solver.train()\n",
    "            trains+=1\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "        dqn_solver.decrement_epsilon()\n",
    "        scores.append(score)\n",
    "        no_train.append(trains)             # save most recent score\n",
    "        if(i_episode%5==0):\n",
    "            dqn_solver.update_target_model()\n",
    "        if(i_episode%20==0):\n",
    "            print('\\rEpisode {}\\tScore: {:.2f}'.format(i_episode, score, end=\"\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2:   6%|â–Œ         | 12/200 [00:00<00:05, 33.96it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Model was constructed with shape (None, 1, 2) for input KerasTensor(type_spec=TensorSpec(shape=(None, 1, 2), dtype=tf.float32, name='input_1'), name='input_1', description=\"created by layer 'input_1'\"), but it was called on an input with incompatible shape (1, 2).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 20\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 40\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 60\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 80\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 120\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 140\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 160\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 180\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 200\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 220\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 240\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 260\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 280\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 300\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 320\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 340\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 360\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 380\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 400\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 420\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 440\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 460\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 480\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 500\tScore: -154.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 520\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 540\tScore: -144.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 560\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 580\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 600\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 620\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 640\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 660\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 680\tScore: -166.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 700\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 720\tScore: -196.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 740\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 760\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 780\tScore: -116.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 800\tScore: -168.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 820\tScore: -200.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 840\tScore: -152.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 860\tScore: -148.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 880\tScore: -173.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 900\tScore: -85.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 920\tScore: -146.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 940\tScore: -148.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 960\tScore: -127.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 980\tScore: -117.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                       "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1000\tScore: -118.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "training(dqn_solver)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
